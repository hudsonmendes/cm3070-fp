{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM3070, Exploratory Data Analysis\n",
    "\n",
    "```\n",
    "University of London\n",
    "BSc Computer Science\n",
    "CM3070, Final Project\n",
    "Hudson Leonardo MENDES\n",
    "hlm12@student.london.ac.uk\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T14:43:02.985547Z",
     "start_time": "2023-07-13T14:43:02.842918Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat ../setup.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T14:43:03.141254Z",
     "start_time": "2023-07-13T14:43:03.092297Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T14:43:03.487527Z",
     "start_time": "2023-07-13T14:43:03.452824Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Paths & Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T14:43:03.870117Z",
     "start_time": "2023-07-13T14:43:03.837574Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dir_data = pathlib.Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction, Transformation & Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T14:43:07.117036Z",
     "start_time": "2023-07-13T14:43:04.961415Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -e '..[etl]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. [E]xtraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T14:46:22.459843Z",
     "start_time": "2023-07-13T14:46:21.635307Z"
    }
   },
   "outputs": [],
   "source": [
    "from hlm12erc.etl import ETL, KaggleDataset\n",
    "\n",
    "ds_kaggle = KaggleDataset(owner=\"zaber666\", name=\"meld-dataset\", subdir=\"MELD-RAW/MELD.Raw\")\n",
    "etl = ETL(dataset=ds_kaggle).into(uri_or_folderpath=dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {str(dir_data)} | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(dir_data / \"train.csv\", index_col=0)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "df_sample = df_raw.groupby([\"label\"], group_keys=False).apply(lambda x: x.sample(min(len(x), 3)))\n",
    "df_sample = df_sample.sort_values([\"label\"])\n",
    "\n",
    "table_rows = []\n",
    "for i, row in df_sample.iterrows():\n",
    "    speaker_cell = f'<td>{row[\"speaker\"]}</td>'\n",
    "    text_cell = f'<td>{row[\"x_text\"]}</td>'\n",
    "    image_path = dir_data / row[\"x_visual\"]\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        crop_top = height // 2 - height // 10\n",
    "        crop_bottom = height // 2 + height // 10\n",
    "        img_cropped = img.crop((0, crop_top, width, crop_bottom))\n",
    "        buffer = io.BytesIO()\n",
    "        img_cropped.save(buffer, format=\"JPEG\")\n",
    "        image_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "    image_cell = f'<td><img src=\"data:image/jpeg;base64,{image_data}\" width=\"100\"></td>'\n",
    "    audio_cell = f'<td><audio controls src=\"{dir_data / row[\"x_audio\"]}\" /></td>'\n",
    "    label_cell = f'<td>{row[\"label\"]}</td>'\n",
    "    table_rows.append(f\"<tr>{speaker_cell}{text_cell}{image_cell}{audio_cell}{label_cell}</tr>\")\n",
    "\n",
    "table_html = (\n",
    "    \"<table><tr><th>Speaker</th><th>Text</th><th>Image</th><th>Audio</th><th>Emotion</th></tr>\"\n",
    "    + \"\".join(table_rows)\n",
    "    + \"</table>\"\n",
    ")\n",
    "display(HTML(table_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    dict(\n",
    "        max_char_length=max(df_raw.x_text.map(lambda x: len(x))),\n",
    "        max_token_count=max(df_raw.x_text.map(lambda x: len(word_tokenize(x)))),\n",
    "        count_label_neutral=len(df_raw[df_raw.label == \"neutral\"]),\n",
    "        count_label_joy=len(df_raw[df_raw.label == \"joy\"]),\n",
    "        count_label_sadness=len(df_raw[df_raw.label == \"sadness\"]),\n",
    "        count_label_fear=len(df_raw[df_raw.label == \"fear\"]),\n",
    "        count_label_anger=len(df_raw[df_raw.label == \"anger\"]),\n",
    "        count_label_surprise=len(df_raw[df_raw.label == \"surprise\"]),\n",
    "        count_label_disgust=len(df_raw[df_raw.label == \"disgust\"]),\n",
    "    ),\n",
    "    orient=\"index\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Measures of Spread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Types of Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Wordclouds (with & without Stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Combine all text into a single string and remove special chars\n",
    "text = re.sub(r\"[^\\w\\s\\d]\", \"\", \" \".join(df_raw.x_text.tolist()).lower())\n",
    "\n",
    "# Generate the word cloud with stopwords\n",
    "wordcloud_with_stopwords = WordCloud(width=750, height=1000, background_color=\"white\").generate(text)\n",
    "\n",
    "# Remove stopwords from the text\n",
    "text_without_stopwords = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Generate the word cloud without stopwords\n",
    "wordcloud_without_stopwords = WordCloud(width=750, height=1000, background_color=\"white\").generate(\n",
    "    text_without_stopwords\n",
    ")\n",
    "\n",
    "# Display the word clouds side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(wordcloud_with_stopwords, interpolation=\"bilinear\")\n",
    "axs[0].set_title(\"with Stopwords\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(wordcloud_without_stopwords, interpolation=\"bilinear\")\n",
    "axs[1].set_title(\"WITHOUT Stopwords\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Sentence Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Tokenize the sentences using the nltk tokenizer\n",
    "df_raw[\"tokens\"] = df_raw[\"x_text\"].apply(nltk.word_tokenize)\n",
    "df_raw[\"sentence_length\"] = df_raw[\"tokens\"].apply(len)\n",
    "\n",
    "# Get the unique emotion labels\n",
    "labels = df_raw[\"label\"].unique()\n",
    "\n",
    "# Create a grid of histograms\n",
    "num_rows = (len(labels) + 3) // 4\n",
    "fig, axs = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))\n",
    "for i, label in enumerate(labels):\n",
    "    # Filter the data by label\n",
    "    df = df_raw[df_raw[\"label\"] == label][\"sentence_length\"]\n",
    "\n",
    "    # Plot the histogram\n",
    "    axs[i // 4, i % 4].hist(df, bins=50)\n",
    "    axs[i // 4, i % 4].set_title(label)\n",
    "    axs[i // 4, i % 4].set_xlabel(\"Sentence Length (Number of Tokens)\")\n",
    "    axs[i // 4, i % 4].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Set the x-axis limit to the maximum sentence length\n",
    "    axs[i // 4, i % 4].set_xlim([0, df_raw[\"sentence_length\"].max()])\n",
    "\n",
    "# Plot the combined distribution\n",
    "df = df_raw[\"sentence_length\"]\n",
    "axs[-1, -1].hist(df, bins=50, color=\"orange\")\n",
    "axs[-1, -1].set_title(\"[all emotions]\")\n",
    "axs[-1, -1].set_xlabel(\"Sentence Length (Number of Tokens)\")\n",
    "axs[-1, -1].set_ylabel(\"Frequency\")\n",
    "axs[-1, -1].set_xlim([0, df_raw[\"sentence_length\"].max()])\n",
    "\n",
    "# Remove the unused subplots\n",
    "for i in range(len(labels) + 1, num_rows * 4):\n",
    "    axs.flat[i].set_visible(False)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. LDA-based Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set the logging level to WARNING\n",
    "gensim.models.ldamodel.logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Tokenize the sentences using the nltk tokenizer, lowercase them, remove special chars and remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "utterances = [re.sub(r\"[^\\w\\s\\d]\", \"\", utterance) for utterance in df_raw[\"x_text\"]]\n",
    "tokens = [nltk.word_tokenize(x) for x in utterances]\n",
    "tokens = [[word.lower() for word in sentence] for sentence in tokens]\n",
    "tokens = [[word for word in sentence if word not in stop_words] for sentence in tokens]\n",
    "\n",
    "# Create a dictionary from the tokens\n",
    "dictionary = gensim.corpora.Dictionary(tokens)\n",
    "\n",
    "# Create a bag-of-words corpus from the dictionary and tokens\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in tokens]\n",
    "\n",
    "# Train the LDA model on the corpus\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    per_word_topics=True,\n",
    ")\n",
    "\n",
    "# Create an interactive visualization of the topics and their associated keywords\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 3D PCA Embedding Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Loading USE4\n",
    "use4_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Loading SimCSE\n",
    "simcse_model_name = \"ZurichNLP/unsup-simcse-xlm-roberta-base\"\n",
    "simcse_tokenizer = AutoTokenizer.from_pretrained(simcse_model_name)\n",
    "simcse_model = AutoModel.from_pretrained(simcse_model_name)\n",
    "\n",
    "# Loading ResNet50\n",
    "resnet50_model = torch.hub.load(\"pytorch/vision:v0.9.0\", \"resnet50\", pretrained=True)\n",
    "resnet50_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.2706, 0.2010, 0.1914], std=[0.1857, 0.1608, 0.1667]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "def plot_3d_embeddings_using(\n",
    "    ax,\n",
    "    title: str,\n",
    "    df: pd.DataFrame,\n",
    "    p: str,\n",
    "    emb_fn: callable,\n",
    "    bsz: int,\n",
    "):\n",
    "    # Get the embeddings for each utterance in df_raw.x_text\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(df), bsz), desc=title):\n",
    "        batch = list(df[p][i : i + bsz])\n",
    "        batch = emb_fn(batch)\n",
    "        embeddings.append(batch)\n",
    "    embeddings = list(itertools.chain(*[e.tolist() for e in embeddings]))\n",
    "    embeddings = pad_sequences(embeddings, dtype=\"float32\", padding=\"post\")\n",
    "\n",
    "    # Perform PCA on the normalized embeddings to reduce them to 3 dimensions\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Normalize the embeddings to lie on the surface of the unit sphere\n",
    "    norms = np.linalg.norm(pca_embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = pca_embeddings / norms\n",
    "\n",
    "    # Create a 3D scatter plot of the PCA embeddings, color-coded by label\n",
    "    colors = dict(\n",
    "        neutral=\"gray\",\n",
    "        surprise=\"yellow\",\n",
    "        fear=\"black\",\n",
    "        sadness=\"blue\",\n",
    "        joy=\"green\",\n",
    "        disgust=\"purple\",\n",
    "        anger=\"red\",\n",
    "    )\n",
    "    for i, label in enumerate(list(df.label)):\n",
    "        ax.scatter(\n",
    "            normalized_embeddings[i, 0],\n",
    "            normalized_embeddings[i, 1],\n",
    "            normalized_embeddings[i, 2],\n",
    "            c=colors[label],\n",
    "        )\n",
    "\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# USE4 Embeddings\n",
    "embedding_fn_use4 = use4_model\n",
    "\n",
    "\n",
    "# SimCSE Embeddings\n",
    "def embedding_fn_simcse(x):\n",
    "    inputs = simcse_tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = simcse_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.numpy()\n",
    "\n",
    "\n",
    "# ResNet50 Embeddings\n",
    "def embedding_fn_resnet50(image_paths):\n",
    "    img_tensors = []\n",
    "    for image_path in image_paths:\n",
    "        img = Image.open(str(dir_data / image_path))\n",
    "        img_tensor = resnet50_transforms(img)\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        img_tensors.append(img_tensor)\n",
    "    with torch.no_grad():\n",
    "        embeddings = resnet50_model(torch.cat(img_tensors, dim=0))\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, dim=1).squeeze(0)\n",
    "    return embeddings.numpy()\n",
    "\n",
    "\n",
    "# MEL Feature Audio Embeddings\n",
    "def embedding_fn_audiomel(audio_paths):\n",
    "    embeddings = []\n",
    "    for audio_path in audio_paths:\n",
    "        y, sr = librosa.load(str(dir_data / audio_path), sr=16000)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max).flatten()\n",
    "        embeddings.append(log_mel_spec)\n",
    "    embeddings = pad_sequences(embeddings, padding=\"post\", dtype=\"float32\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Plotting PCA vector endpoints for a batch of sentences\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax1 = fig.add_subplot(2, 2, 1, projection=\"3d\")\n",
    "ax2 = fig.add_subplot(2, 2, 2, projection=\"3d\")\n",
    "ax3 = fig.add_subplot(2, 2, 3, projection=\"3d\")\n",
    "ax4 = fig.add_subplot(2, 2, 4, projection=\"3d\")\n",
    "\n",
    "df_sample = df_raw.sample(len(df_raw) // 4)\n",
    "plot_3d_embeddings_using(ax1, title=\"USE4\", df=df_sample, p=\"x_text\", emb_fn=embedding_fn_use4, bsz=64)\n",
    "plot_3d_embeddings_using(ax2, title=\"SimCSE\", df=df_sample, p=\"x_text\", emb_fn=embedding_fn_simcse, bsz=16)\n",
    "plot_3d_embeddings_using(ax3, title=\"ResNet50\", df=df_sample, p=\"x_visual\", emb_fn=embedding_fn_resnet50, bsz=4)\n",
    "plot_3d_embeddings_using(ax4, title=\"AudioMEL\", df=df_sample, p=\"x_audio\", emb_fn=embedding_fn_audiomel, bsz=32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions\n",
    "\n",
    "The visualisation of representations composed by out-of-the box feature extraction mechanism\n",
    "demonstrate clearly how the data examples are scattered in the feature space and give a strong\n",
    "indication of how simplistic separation approaches are unlikely to succesfully separate each\n",
    "emotion class.\n",
    "\n",
    "SimCSE seems to offer, out-of-the box, superior representations for the text modality and, as\n",
    "discussed by the SPCL-ERC-CL[3] paper, it can be further improved by designing a customised\n",
    "objective function that is more suitable for the task at hand, as well as finetuning it in the\n",
    "MELD training data.\n",
    "\n",
    "The visual representations provided <conclude about ResNet50 representations>.\n",
    "\n",
    "MEL Features to represent the audio are <conclude about MEL audio representations>.\n",
    "\n",
    "Without further experimentation, it is not possible to conclude whether the representations\n",
    "employed during this exploratory data analysis are sufficient to separate the emotion classes\n",
    "or if alternartive representations can outform them significantly.\n",
    "\n",
    "As set out for by the project design, both the quality of the representations as well as the\n",
    "quality of the model that fuses these representations will be evaluated, which might hopefully\n",
    "address this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Poria, Soujanya, et al. ‘MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations’. ArXiv [Cs.CL], 2019, http://arxiv.org/abs/1810.02508. arXiv.\n",
    "\n",
    "[2] Chen, Sheng-Yeh, et al. ‘EmotionLines: An Emotion Corpus of Multi-Party Conversations’. ArXiv [Cs.CL], 2018, http://arxiv.org/abs/1802.08379. arXiv.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
